# -*- coding: utf-8 -*-
"""Machine_learning_full.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJ1B-d_hbPBt0npWzJ44EgBT2FOvSMwV
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.cm import rainbow
from matplotlib import rcParams
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix

# read data from cloud
df=pd.read_csv("/content/Midterm_dataset_training.csv")

df.head()

#Data Preprocessing.
dataset = pd.get_dummies(df, columns = ["sex", "cp", "fbs", "restecg", "exang", "slope", "ca", "thal"]) # convert catagories

dataset.head()

# Prepare data
y = dataset["target"]
X = dataset.drop(["target"], axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)

standardScaler = StandardScaler()
columns_to_scale = ["age", "trestbps", "chol", "thalach", "oldpeak"]
scaled_X_train=standardScaler.fit_transform(X_train)
scaled_X_text=standardScaler.transform(X_test)

# Section 2: Machine learning performance comparision
# sklearn classifiers to import
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# model building, predict, accuracy imports
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from IPython.display import display

# Evaluate models
from sklearn import metrics
def evaluate_model(model, x_test, y_test):
    # Predict Test Data 
    y_pred = model.predict(x_test)

    # Calculate accuracy, precision, recall, f1-score, and kappa score
    acc = metrics.accuracy_score(y_test, y_pred)
    prec = metrics.precision_score(y_test, y_pred)
    rec = metrics.recall_score(y_test, y_pred)
    f1 = metrics.f1_score(y_test, y_pred)
    kappa = metrics.cohen_kappa_score(y_test, y_pred)

    # Calculate area under curve (AUC)

    y_pred_proba = model.predict_proba(x_test)[::,1]
    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
    auc = metrics.roc_auc_score(y_test, y_pred_proba)

    # Display confussion matrix
    cm = metrics.confusion_matrix(y_test, y_pred)

    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa, 
            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm}

num_folds = 10
# Build parameters of all classifiers
# model 1
random_forest_params = dict(n_estimators=[5, 10, 15, 20, 25], criterion=['gini', 'entropy'], 
                            max_features=[2, 3, 4, 'auto', 'log2', 'sqrt', None], bootstrap=[False, True]
                            )
# model 2
decision_tree_params = dict(criterion=['gini', 'entropy'], splitter=['best', 'random'], min_samples_split=[2, 3, 4],
                            max_features=[2,3,'auto', 'log2', 'sqrt', None], class_weight=['balanced', None], presort=[False, True])
# model 3
perceptron_params = dict(penalty=[None, 'l2', 'l1', 'elasticnet'], fit_intercept=[False, True], shuffle=[False, True],
                         class_weight=['balanced', None], alpha=[0.0001, 0.00025], max_iter=[30,50,90])
# model 4
svm_params = dict(shrinking=[False, True], degree=[3,4], class_weight=['balanced', None])
# model 5
neural_net_params = dict(activation=['identity', 'logistic', 'tanh', 'relu'], hidden_layer_sizes = [(20,15,10),(30,20,15,10),(16,8,4)], 
                         max_iter=[50,80,150], solver=['adam','lbfgs'], learning_rate=['constant', 'invscaling', 'adaptive'], shuffle=[True, False])
# model 6
log_reg_params = dict(class_weight=['balanced', None], solver=['newton-cg', 'lbfgs', 'liblinear', 'sag'], fit_intercept=[True, False])
# model 7
knn_params = dict(n_neighbors=[2, 3, 5, 10], weights=['uniform', 'distance'],
                  algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'], leaf_size=[5,10,15,20])
# model 8
bagging_params = dict(n_estimators=[5, 12, 15, 20], bootstrap=[False, True])
# model 9
ada_boost_params = dict(n_estimators=[50, 75, 100], algorithm=['SAMME', 'SAMME.R'])
# model 10
guassiannb_params = dict()
#model 11
gradient_boosting_params = dict(n_estimators=[15, 25, 50])

params = [
    random_forest_params, decision_tree_params, perceptron_params,
    svm_params, neural_net_params, log_reg_params, knn_params,
    bagging_params, ada_boost_params, guassiannb_params, gradient_boosting_params
]
# classifiers to test
classifiers = [
    RandomForestClassifier(), DecisionTreeClassifier(), Perceptron(),
    SVC(), MLPClassifier(), LogisticRegression(),
    KNeighborsClassifier(), BaggingClassifier(), AdaBoostClassifier(),
    GaussianNB(), GradientBoostingClassifier()
]

names = [
    'RandomForest', 'DecisionTree', 'Perceptron', 'SVM',
    'NeuralNetwork', 'LogisticRegression',
    'KNearestNeighbors', 'Bagging', 'AdaBoost', 'Naive-Bayes', 'GradientBoosting'
]

models = dict(zip(names, zip(classifiers, params)))

#Finding best parameters using Gridsearch
def parameter_tuning(models, X_train, X_test, y_train, y_test):
    print(num_folds,'fold cross-validation is used')
    print()
    accuracies = []
    compare={}
    # dataframe to store intermediate results
    dataframes = []
    best_parameters = []
    for name, clf_and_params in models.items():
        print('Computing GridSearch on {} '.format(name))
        clf, clf_params = clf_and_params
        # parameter and training
        grid_clf = GridSearchCV(estimator=clf, param_grid=clf_params, cv=num_folds)
        grid_clf = grid_clf.fit(X_train, y_train)
        dataframes.append((name, grid_clf.cv_results_))
        best_parameters.append((name, grid_clf.best_params_))
        # testing
        try:
          compare[name]=evaluate_model(grid_clf,X_test,y_test)
        except: pass
        predictions = grid_clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)
        accuracies.append((name, accuracy, np.mean(cv_scores)))
    return accuracies, dataframes, best_parameters, compare



# Run performance
results, dataframes, best_parameters, compare = parameter_tuning(models, X_train, X_test, y_train, y_test)

# Result
for classifier, acc, cv_acc in results:
    print('{}: Accuracy with Best Parameters = {}% || Mean Cross Validation Accuracy = {}%'.format(classifier, round(acc*100,4), round(cv_acc*100,4)))

# Parameter
# for name, bp in best_parameters:
#     print('============================================================')
#     print('{} classifier GridSearch Best Parameters'.format(name))
#     display(bp)

best_parameters[8]

from matplotlib import pyplot as plt


def bar_plot(ax, data, colors=None, total_width=0.8, single_width=1, legend=True):
    # Check if colors where provided, otherwhise use the default color cycle
    if colors is None:
        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

    # Number of bars per group
    n_bars = len(data)

    # The width of a single bar
    bar_width = total_width / n_bars

    # List containing handles for the drawn bars, used for the legend
    bars = []

    # Iterate over all data
    for i, (name, values) in enumerate(data.items()):
        # The offset in x direction of that bar
        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2

        # Draw a bar for every value of that type
        for x, y in enumerate(values):
            bar = ax.bar(x + x_offset, y, width=bar_width * single_width, color=colors[i % len(colors)])

        # Add a handle to the last drawn bar, which we'll need for the legend
        bars.append(bar[0])

    # Draw legend if we need
    if legend:
        ax.legend(bars, data.keys())

result=pd.DataFrame.from_dict(compare).T

result=result.sort_values(by=["acc","auc","f1","kappa"],ascending=[0,0,0,0])

result.to_csv("result.csv",index=False)

result=pd.read_csv("result.csv")

!pip install matplotlib==3.4

# Note: Preparation for changing data
# df_pivot = pd.pivot_table(
# 	df,
# 	values="page_views",
# 	index="year",
# 	columns="month",
# 	aggfunc=np.mean
# )

# Section 3: Visualization 
# Plot a bar chart using the DF
ax = result.plot(kind="bar",width=0.7)
# Get a Matplotlib figure from the axes object for formatting purposes
fig = ax.get_figure()
# Change the plot dimensions (width, height)
fig.set_size_inches(30, 5)
# Change the axes labels
ax.set_xlabel("Years")
ax.set_ylabel("Average Page Views")
ax
# Use this to show the plot in a new window
# plt.show()
# Export the plot as a PNG file
for container in ax.containers:
    ax.bar_label(container)

#Single model

from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix, accuracy_score

model=AdaBoostClassifier(algorithm='SAMME',n_estimators=75)
best_model=model.fit(scaled_X_train,y_train)

plot_confusion_matrix(best_model,scaled_X_text,y_test)

from sklearn.metrics import plot_precision_recall_curve,plot_roc_curve

plot_precision_recall_curve(best_model,scaled_X_text,y_test)

plot_roc_curve(best_model,scaled_X_text,y_test)

"""**Final Task: A patient with the following features has come into the medical office:**

    age          48.0
    sex           0.0
    cp            2.0
    trestbps    130.0
    chol        275.0
    fbs           0.0
    restecg       1.0
    thalach     139.0
    exang         0.0
    oldpeak       0.2
    slope         2.0
    ca            0.0
    thal          2.0
"""

X.columns

test=[[48,130,275,139,0.2, 1,0 ,0,0,1,0 ,1,0 ,0,1,0 ,1,0 ,0,0,1, 1,0,0,0,0, 0,0,1,0 ]]

test=standardScaler.transform(test)

best_model.predict(test)

df[df["age"]==48]

# Save and load model
import joblib
import pickle
joblib.dump(best_model,"sale_predict_2_degree.sav")