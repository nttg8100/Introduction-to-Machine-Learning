# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KXPz4U9qpAGloVemaMij1ONBPm64G5xA
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.cm import rainbow
from matplotlib import rcParams
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import LabelEncoder

# training
df=pd.read_csv("/content/drive/MyDrive/Midterm_dataset_training.csv")

"""# 1 Data preprocess
Drug discovery takes a long journey from the beginning to the end which is more important than drug discovery data for the gene expression 
"""

# Get the information from data
df.info()

# Describe the information of data
df.describe()

# Get 5 first lines in the data
df.head()

# lb_make = LabelEncoder()
# df["target"] = lb_make.fit_transform(df["Bioactivity"])

# Convert the information to Biodactiv
df["target"]=df["Bioactivity"].apply(lambda x: 1 if x=="active" else 0)

# Prepare data
y = df["target"]
X = df.drop(["target","Name","Bioactivity"], axis = 1)

y.value_counts()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

ros = RandomOverSampler(random_state=42) 
rus = RandomUnderSampler(random_state=42)
sm=SMOTE(random_state=42)

x_ros, y_ros = ros.fit_resample(X_train, y_train)
x_sm, y_sm = sm.fit_resample(X_train, y_train)
x_rus, y_rus = rus.fit_resample(X_train, y_train) #undersampling data

# Function for select features
def select_features(X_train, y_train, X_test,df):
  fs = SelectKBest(score_func=chi2, k="all")
  fs.fit(X_train, y_train)
  X_train_fs = fs.transform(X_train)
  X_test_fs = fs.transform(X_test)   # Feature for sorting the levels of important features
  feature_names =list(df.columns.values)
  mask = fs.get_support() #list of booleans
  result_features = {} # The list of your K best features
  for feature,score in zip(feature_names,fs.scores_):
      result_features[feature]=score
  scores=pd.DataFrame(result_features,index=["Score"]).T.dropna()
  scores=scores.sort_values(by="Score",ascending=0)
  scores=scores[scores["Score"]>3]#select the significant features with high score
  # scores=scores.head(20) #Select 20 best features
  # scores=scores.head(5)
  print(scores)
  X_train_fs=X_train[scores.index]
  X_test_fs =X_test[scores.index]   # Feature for sorting the levels of important features
  return X_train_fs,X_test_fs,scores

# Preprocess data in 3 ways
x_ros_train_fs,x_ros_test_fs,scores_ros=select_features(x_ros,y_ros,X_test,X)
x_sm_train_fs,x_sm_test_fs,score_sm=select_features(x_sm,y_sm,X_test,X)
x_rus_train_fs,x_rus_test_fs,score_rus=select_features(x_rus,y_rus,X_test,X)

# Get the sise of preprocess data
print(x_ros_train_fs.shape,x_ros_test_fs.shape)
print(x_sm_train_fs.shape,x_sm_test_fs.shape)
print(x_rus_train_fs.shape,x_rus_test_fs.shape)

# Section 2: Machine learning performance comparision
# sklearn classifiers to import
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# model building, predict, accuracy imports
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from IPython.display import display

# Evaluate models
from sklearn import metrics
def evaluate_model(model, x_test, y_test):
    # Predict Test Data 
    try:
      y_pred = model.predict(x_test)

      # Calculate accuracy, precision, recall, f1-score, and kappa score
      acc = metrics.accuracy_score(y_test, y_pred)
      prec = metrics.precision_score(y_test, y_pred)
      rec = metrics.recall_score(y_test, y_pred)
      f1 = metrics.f1_score(y_test, y_pred)
      kappa = metrics.cohen_kappa_score(y_test, y_pred)

      # Calculate area under curve (AUC)

      y_pred_proba = model.predict_proba(x_test)[::,1]
      fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
      auc = metrics.roc_auc_score(y_test, y_pred_proba)

      # Display confussion matrix
      cm = metrics.confusion_matrix(y_test, y_pred)
      result={'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa, 
            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm}
    except: result="null"
    return result

# Settings the parameters for all of the models
num_folds = 5
# Build parameters of all classifiers
# model 1
random_forest_params = dict(n_estimators=[5, 10, 15, 20, 25], criterion=['gini', 'entropy'], 
                            max_features=[2, 3, 4, 'auto', 'log2', 'sqrt', None], bootstrap=[False, True]
                            )
# model 2
decision_tree_params = dict(criterion=['gini', 'entropy'], splitter=['best', 'random'], min_samples_split=[2, 3, 4],
                            max_features=[2,3,'auto', 'log2', 'sqrt', None], class_weight=['balanced', None], presort=[False, True])
# model 3
perceptron_params = dict(penalty=[None, 'l2', 'l1', 'elasticnet'], fit_intercept=[False, True], shuffle=[False, True],
                         class_weight=['balanced', None], alpha=[0.0001, 0.00025], max_iter=[30,50,90])
# model 4
svm_params = dict(shrinking=[False, True], degree=[3,4], class_weight=['balanced', None])
# model 5
neural_net_params = dict(activation=['identity', 'logistic', 'tanh', 'relu'], hidden_layer_sizes = [(20,15,10),(30,20,15,10),(16,8,4)], 
                         max_iter=[50,80,150], solver=['adam','lbfgs'], learning_rate=['constant', 'invscaling', 'adaptive'], shuffle=[True, False])
# model 6
log_reg_params = dict(class_weight=['balanced', None], solver=['newton-cg', 'lbfgs', 'liblinear', 'sag'], fit_intercept=[True, False])
# model 7
knn_params = dict(n_neighbors=[2, 3, 5, 10], weights=['uniform', 'distance'],
                  algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'], leaf_size=[5,10,15,20])
# model 8
bagging_params = dict(n_estimators=[5, 12, 15, 20], bootstrap=[False, True])
# model 9
ada_boost_params = dict(n_estimators=[50, 75, 100], algorithm=['SAMME', 'SAMME.R'])
# model 10
guassiannb_params = dict()
#model 11
gradient_boosting_params = dict(n_estimators=[15, 25, 50])

# Using the GridSearch to filter out the best paramters for the models. However, it takes over 12 hours that GG colab can not fulfill the whole process. 
# Return to compare only on the sections of parameter. Ignore this sections
params = [
    random_forest_params, decision_tree_params, perceptron_params,
    svm_params, neural_net_params, log_reg_params, knn_params,
    bagging_params, ada_boost_params, guassiannb_params, gradient_boosting_params
]
# classifiers to test
classifiers = [
    RandomForestClassifier(), DecisionTreeClassifier(), Perceptron(),
    SVC(), MLPClassifier(), LogisticRegression(),
    KNeighborsClassifier(), BaggingClassifier(), AdaBoostClassifier(),
    GaussianNB(), GradientBoostingClassifier()
]

names = [
    'RandomForest', 'DecisionTree', 'Perceptron', 'SVM',
    'NeuralNetwork', 'LogisticRegression',
    'KNearestNeighbors', 'Bagging', 'AdaBoost', 'Naive-Bayes', 'GradientBoosting'
]

models = dict(zip(names, zip(classifiers, params)))

#Finding best parameters using Gridsearch
def parameter_tuning(models, X_train, X_test, y_train, y_test):
    print(num_folds,'fold cross-validation is used')
    print()
    accuracies = []
    compare={}
    # dataframe to store intermediate results
    dataframes = []
    best_parameters = []
    for name, clf_and_params in models.items():
      try:
        print('Computing GridSearch on {} '.format(name))
        clf, clf_params = clf_and_params
        # parameter and training
        grid_clf = GridSearchCV(estimator=clf, param_grid=clf_params)
        grid_clf = grid_clf.fit(X_train, y_train)
        dataframes.append((name, grid_clf.cv_results_))
        best_parameters.append((name, grid_clf.best_params_))
        # testing
        compare[name]=evaluate_model(grid_clf,X_test,y_test)
        predictions = grid_clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)
        accuracies.append((name, accuracy, np.mean(cv_scores)))
      except: pass
    return accuracies, dataframes, best_parameters, compare

# Run without optimization
models = dict(zip(names, zip(classifiers, params)))
# models = dict(zip([names[0]], zip([classifiers[0]], [params[0]])))
#Finding best parameters using Gridsearch
def parameter_tuning(models, X_train, X_test, y_train, y_test):
    print(num_folds,'fold cross-validation is used')
    print()
    accuracies = []
    compare={}
    # dataframe to store intermediate results
    for name, clf_and_params in models.items():
        print('Computing Model on {} '.format(name))
        clf, clf_params = clf_and_params
        print(clf)
        # parameter and training
        grid_clf = clf.fit(X_train, y_train)
        # testing
        try:
          compare[name]=evaluate_model(grid_clf,X_test,y_test)
        except:
          compare[name]="null"
        predictions = grid_clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)
        accuracies.append((name, accuracy, np.mean(cv_scores)))
    return accuracies,compare

# Run performance
# results_1,frame_1,para_1,compare_1 = parameter_tuning(models,x_ros_train_fs,x_ros_test_fs, y_ros, y_test)
results_1,compare_1 = parameter_tuning(models,x_ros_train_fs,x_ros_test_fs, y_ros, y_test)

# Run performance
# results_2,frame_2,para_2,compare_2 = parameter_tuning(models,x_sm_train_fs,x_sm_test_fs, y_sm, y_test)
results_2, compare_2 = parameter_tuning(models,x_sm_train_fs,x_sm_test_fs, y_sm, y_test)

# Run performance
# results_3,frame_3,para_3,compare_3 = parameter_tuning(models,x_rus_train_fs,x_rus_test_fs, y_rus, y_test)
results_3, compare_3 = parameter_tuning(models,x_rus_train_fs,x_rus_test_fs, y_rus, y_test)

performance_1=pd.DataFrame.from_dict(compare_1).T
performance_1.sort_values(by=["acc","auc","f1","kappa"],ascending=[0,0,0,0])

performance_2=pd.DataFrame.from_dict(compare_2).T
performance_2.sort_values(by=["acc","auc","f1","kappa"],ascending=[0,0,0,0])

performance_3=pd.DataFrame.from_dict(compare_3).T
performance_3.sort_values(by=["acc","auc","f1","kappa"],ascending=[0,0,0,0])

# Result from 3 diffferent preprocess data
for classifier, acc, cv_acc in results_1:
    print('{}: Accuracy with Best Parameters = {}% || Mean Cross Validation Accuracy = {}%'.format(classifier, round(acc*100,4), round(cv_acc*100,4)))

for classifier, acc, cv_acc in results_2:
    print('{}: Accuracy with Best Parameters = {}% || Mean Cross Validation Accuracy = {}%'.format(classifier, round(acc*100,4), round(cv_acc*100,4)))

# Result
for classifier, acc, cv_acc in results_3:
    print('{}: Accuracy with Best Parameters = {}% || Mean Cross Validation Accuracy = {}%'.format(classifier, round(acc*100,4), round(cv_acc*100,4)))

# Run performance
# results_2, dataframes_2, best_parameters_2, compare_2 = parameter_tuning(models,x_ros_train_fs,x_ros_test_fs, y_ros, y_test)

x_ros_train_fs.shape

model=RandomForestClassifier()
result=model.fit(x_ros_train_fs,y_ros)
from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix, accuracy_score
plot_confusion_matrix(result,x_ros_test_fs,y_test)